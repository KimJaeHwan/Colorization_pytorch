{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import torchvision.utils as v_utils\n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_dim, out_dim, act_fn):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_dim,out_dim, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        act_fn\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_trans_block(in_dim, out_dim, act_fn):\n",
    "    model = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        act_fn\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool():\n",
    "    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block_2(in_dim,out_dim,act_fn):\n",
    "    model = nn.Sequential(\n",
    "        conv_block(in_dim, out_dim, act_fn),\n",
    "        nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_dim)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetGenerator(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,num_filter):\n",
    "        super(UnetGenerator,self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_filter = num_filter\n",
    "        act_fn = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        print(\"\\n------Initiating U-Net------\\n\")\n",
    "\n",
    "        self.down_1 = conv_block_2(self.in_dim,self.num_filter,act_fn)\n",
    "        self.pool_1 = maxpool()\n",
    "        self.down_2 = conv_block_2(self.num_filter*1,self.num_filter*2,act_fn)\n",
    "        self.pool_2 = maxpool()\n",
    "        self.down_3 = conv_block_2(self.num_filter*2,self.num_filter*4,act_fn)\n",
    "        self.pool_3 = maxpool()\n",
    "        self.down_4 = conv_block_2(self.num_filter*4,self.num_filter*8,act_fn)\n",
    "        self.pool_4 = maxpool()\n",
    "\n",
    "        self.bridge = conv_block_2(self.num_filter*8,self.num_filter*16,act_fn)\n",
    "\n",
    "        self.trans_1 = conv_trans_block(self.num_filter*16,self.num_filter*8,act_fn)\n",
    "        self.up_1 = conv_block_2(self.num_filter*16,self.num_filter*8,act_fn)\n",
    "        self.trans_2 = conv_trans_block(self.num_filter*8,self.num_filter*4,act_fn)\n",
    "        self.up_2 = conv_block_2(self.num_filter*8,self.num_filter*4,act_fn)\n",
    "        self.trans_3 = conv_trans_block(self.num_filter*4,self.num_filter*2,act_fn)\n",
    "        self.up_3 = conv_block_2(self.num_filter*4,self.num_filter*2,act_fn)\n",
    "        self.trans_4 = conv_trans_block(self.num_filter*2,self.num_filter*1,act_fn)\n",
    "        self.up_4 = conv_block_2(self.num_filter*2,self.num_filter*1,act_fn)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(self.num_filter,self.out_dim,3,1,1),\n",
    "            nn.Tanh(),  #필수는 아님\n",
    "        )\n",
    "        \n",
    "    def forward(self,input):\n",
    "        down_1 = self.down_1(input)\n",
    "        pool_1 = self.pool_1(down_1)\n",
    "        down_2 = self.down_2(pool_1)\n",
    "        pool_2 = self.pool_2(down_2)\n",
    "        down_3 = self.down_3(pool_2)\n",
    "        pool_3 = self.pool_3(down_3)\n",
    "        down_4 = self.down_4(pool_3)\n",
    "        pool_4 = self.pool_4(down_4)\n",
    "\n",
    "        bridge = self.bridge(pool_4)\n",
    "\n",
    "        trans_1 = self.trans_1(bridge)\n",
    "        concat_1 = torch.cat([trans_1,down_4],dim=1)\n",
    "        up_1 = self.up_1(concat_1)\n",
    "        trans_2 = self.trans_2(up_1)\n",
    "        concat_2 = torch.cat([trans_2,down_3],dim=1)\n",
    "        up_2 = self.up_2(concat_2)\n",
    "        trans_3 = self.trans_3(up_2)\n",
    "        concat_3 = torch.cat([trans_3,down_2],dim=1)\n",
    "        up_3 = self.up_3(concat_3)\n",
    "        trans_4 = self.trans_4(up_3)\n",
    "        concat_4 = torch.cat([trans_4,down_1],dim=1)\n",
    "        up_4 = self.up_4(concat_4)\n",
    "        out = self.out(up_4)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim=3,hidden_dim=16):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        act_fn = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.disc = nn.Sequential(\n",
    "            self.disc_block(in_dim,hidden_dim,kernel_size=4,stride=2,act_fn=act_fn),                # 16 * 127 * 127\n",
    "            self.disc_block(hidden_dim,hidden_dim * 2,kernel_size=5,stride=2,act_fn=act_fn),        # 32 * 62 * 62\n",
    "            self.disc_block(hidden_dim * 2,hidden_dim * 4,kernel_size=4,stride=2,act_fn=act_fn),    # 64 * 30 * 30\n",
    "            self.disc_block(hidden_dim * 4,hidden_dim * 8,kernel_size=4,stride=2,act_fn=act_fn),    # 128 * 14 * 14\n",
    "            self.disc_block(hidden_dim * 8,hidden_dim * 16,kernel_size=5,stride=3,act_fn=act_fn),    # 256 * 4 * 4\n",
    "            self.disc_block(hidden_dim * 16,1,kernel_size=4,stride=1,act_fn=act_fn, final_layer=True),    # 1 * 1 * 1\n",
    "        )\n",
    "    def disc_block(self, in_dim, out_dim,kernel_size,stride, act_fn, final_layer=False):\n",
    "        if final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_dim,out_dim,kernel_size,stride)\n",
    "            )\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_dim,out_dim,kernel_size,stride),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            act_fn\n",
    "        )\n",
    "    def forward(self, image):\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "img_size = 256\n",
    "in_dim = 1\n",
    "out_dim = 3\n",
    "num_filters = 16\n",
    "\n",
    "sample_input = torch.ones(size=(batch_size,1,img_size, img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "transform=transforms.Compose([\n",
    "                            transforms.Resize(256),\n",
    "                            transforms.CenterCrop(256),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 이미지 값 [-1,1] 사이로 변환\n",
    "                        ])\n",
    "transform_gray=transforms.Compose([\n",
    "                            transforms.Grayscale(num_output_channels=1),\n",
    "                            transforms.Resize(256),\n",
    "                            transforms.CenterCrop(256),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5), (0.5)), # 이미지 값 [-1,1] 사이로 변환\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_color = datasets.ImageFolder(\"./colorization_data/color_image\", transform=transform)\n",
    "dataset_gray = datasets.ImageFolder(\"./colorization_data/color_image\", transform=transform_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_color_train , dataset_color_test = train_test_split(dataset_color, test_size=0.2, random_state=42)\n",
    "dataset_gray_train, dataset_gray_test = train_test_split(dataset_gray,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_color_loader = DataLoader(dataset_color_train, batch_size=batch_size)\n",
    "train_gray_loader = DataLoader(dataset_gray_train, batch_size=batch_size)\n",
    "test_color_loader = DataLoader(dataset_color_test, batch_size=batch_size)\n",
    "test_gray_loader = DataLoader(dataset_gray_test , batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=25, size=(3,64,64)):\n",
    "    '''\n",
    "    image_tensor : 텐서로 구성된 이미지\n",
    "    num_images : 이미지 개수\n",
    "    size : 이미지 크기\n",
    "    -> 이미지를 화면에 보여준다.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = v_utils.make_grid(image_unflat[:num_images], nrow=4)\n",
    "    plt.imshow(image_grid.permute(1,2,0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Initiating U-Net------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_step = 50\n",
    "Learning_rate = 0.0001\n",
    "n_epochs = 100\n",
    "\n",
    "Unet = UnetGenerator(in_dim=in_dim, out_dim=out_dim,num_filter=num_filters).to(device)\n",
    "disc = Discriminator().to(device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "unet_optimizer = torch.optim.Adam(Unet.parameters(), lr=Learning_rate)\n",
    "disc_optimizer = torch.optim.Adam(disc.parameters(), lr=Learning_rate)\n",
    "\n",
    "Unet = Unet.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_disc_loss = 0\n",
    "mean_unet_loss = 0\n",
    "cur_step = 0\n",
    "display_step = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch + 1}/{n_epochs}')\n",
    "    for [color , _],[gray , _] in tqdm(zip(train_color_loader, train_gray_loader)):\n",
    "        disc_optimizer.zero_grad()\n",
    "        gray = gray.to(device)\n",
    "        color = color.to(device)\n",
    "\n",
    "        output = Unet(gray)\n",
    "        logits_output = disc(output.detach())\n",
    "        logits_color = disc(color)\n",
    "\n",
    "        disc_loss_output = loss_func(logits_output, torch.zeros_like(logits_output))\n",
    "        disc_loss_color = loss_func(logits_color, torch.ones_like(logits_color))\n",
    "        disc_loss = (disc_loss_output + disc_loss_color) / 2\n",
    "\n",
    "        mean_disc_loss += disc_loss.item()\n",
    "        \n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        unet_optimizer.zero_grad()\n",
    "\n",
    "        output = Unet(gray)\n",
    "        logits_output = disc(output)\n",
    "\n",
    "        unet_loss = loss_func(logits_output, torch.ones_like(logits_output))\n",
    "\n",
    "        mean_unet_loss += unet_loss.item() \n",
    "\n",
    "        unet_loss.backward()\n",
    "        unet_optimizer.step()\n",
    "\n",
    "        if cur_step%display_step == 0 or cur_step == 0 or epoch == n_epochs :\n",
    "            print(f\"Step {cur_step}: Generator loss: {mean_unet_loss / display_step}, discriminator loss: {mean_disc_loss / display_step}\")\n",
    "            show_tensor_images(output, num_images=16, size=(3,256,256))\n",
    "            show_tensor_images(color,num_images=16, size=(3,256,256))\n",
    "            mean_unet_loss = 0\n",
    "            mean_disc_loss = 0\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tensor_images(image_tensor, num_images=25, size=(3,64,64), dir=\"\", num=0):\n",
    "    '''\n",
    "    image_tensor : 텐서로 구성된 이미지\n",
    "    num_images : 이미지 개수\n",
    "    size : 이미지 크기\n",
    "    -> 이미지를 화면에 보여준다.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    #image_grid = v_utils.make_grid(image_unflat[:num_images], nrow=4)\n",
    "    #print(image_unflat[0].permute(1,2,0).squeeze().shape)\n",
    "    for i in range(num_images):\n",
    "        image_name = f'{num}_{i}.jpg'\n",
    "        v_utils.save_image(image_unflat[i].squeeze(), os.path.join(dir,image_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_path = \"./output_image_GAN_200/gray/\"\n",
    "color_path = \"./output_image_GAN_200/color/\"\n",
    "output_path = \"./output_image_GAN_200/output/\"\n",
    "\n",
    "gray_image = []\n",
    "color_image = []\n",
    "output_image = []\n",
    "display_step = 10\n",
    "with torch.no_grad():\n",
    "    cur_step = 0\n",
    "    for i,[[color, _] ,[gray, _]] in tqdm(enumerate(zip(test_color_loader, test_gray_loader))):\n",
    "        color = color.to(device)\n",
    "        gray = gray.to(device)\n",
    "\n",
    "        output = Unet(gray)\n",
    "        ''''\n",
    "        gray_image.append(torch.squeeze(gray.cpu().data))\n",
    "        color_image.append(torch.squeeze(color.cpu().data))\n",
    "        output_image.append(torch.squeeze(output.cpu().data))\n",
    "        '''\n",
    "        if cur_step%display_step == 0 or cur_step == 0:\n",
    "            show_tensor_images(output, num_images=16, size=(3,256,256))\n",
    "            show_tensor_images(color,num_images=16, size=(3,256,256))\n",
    "            show_tensor_images(gray,num_images=16, size=(256,256))\n",
    "        \n",
    "        save_tensor_images(gray,num_images=16, size=(256,256),dir=gray_path,num=cur_step)\n",
    "        save_tensor_images(color,num_images=16, size=(3,256,256),dir=color_path,num=cur_step)\n",
    "        save_tensor_images(output,num_images=16, size=(3,256,256),dir=output_path,num=cur_step)\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Unet,\"./model/colorization_GAN_200_Unet.pt\")\n",
    "torch.save(disc, \"./model/colorization_GAN_200_disc.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "267afb14fdf7b26847449cf4f7f0cd28ba2362ced1c6cba84d03c7f1c52756fc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
